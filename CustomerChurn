{"noteParams":{},"angularObjects":{"md:shared_process":[],"Notebook__CustomerChurn__CustomerChurn__1636587344394:shared_process":[]},"name":"CustomerChurn","noteForms":{},"id":"2GK7J6WPR","paragraphs":[{"jobName":"paragraph_1636587344418_195478516","settings":{"params":{},"forms":{}},"dateFinished":"2021-11-10 23:42:32.780","progressUpdateIntervalMs":500,"dateUpdated":"2021-11-10 23:41:24.340","dateCreated":"2021-11-10 23:35:44.418","dateStarted":"2021-11-10 23:41:24.351","isDefaultParagraph":false,"text":"%pyspark\n# Matplotlib for visualization\nimport matplotlib\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot as plt\n\n\n# Seaborn for easier visualization\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\n# Import Elastic Net, Ridge Regression, and Lasso Regression\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\n\n# Import Random Forest and Gradient Boosted Trees\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# Import Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Import RandomForestClassifier and GradientBoostingClassifer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Function for creating model pipelines\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# For standardization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\n\n# Import r2_score and mean_absolute_error functions\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error \n\n# For saving our finalized model\nimport pickle\n\n# For working with dates\nimport datetime\n\n\n# For building linear regression scatter plots\nfrom sklearn import datasets, linear_model\n\n# Classification metrics \nfrom sklearn.metrics import roc_curve, auc\n\n## Clustering analysis\nfrom sklearn.cluster import KMeans\n\n# Import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport pandas as pd \n\nimport pyspark.sql.functions as f\n\ndf = read(\"CustomerChurn.CustomerActivityOverTime\")\ndf.head()\n\n\ndf = df.toPandas()\ndf.head()\n\n\ndf.Account_Name = df.Account_Name.fillna(' ')\ndf.Revenue_Amount = df.Revenue_Amount.fillna(0)\ndf\ndf = df.drop(0)\ndf\ndf1 = df\ndf1 = df1.drop(['Churn'],axis =1)\ndf1.head()\ndf1 = df1.drop(['Account_Name'], axis =1) \ndf1.head()\ndf1 = pd.get_dummies(df1)\ndf1.head()\nx = df1\ny = df.Churn\n# Split X and y into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(x, y, \n                                                    test_size=0.2, \n                                                    random_state=1234,\n                                                    stratify=df.Churn)\n\n# Print number of observations in X_train, X_test, y_train, and y_test\nprint( len(X_train), len(X_test), len(y_train), len(y_test) )\n## We standardized our train data \nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train)\nX_train.head()\n## We standardized our test data \nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X_test)\nX_test = pd.DataFrame(X_test)\nX_test.head()\n# Pipeline dictionary\npipelines = {\n   \n    'gb' : make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n}\n# Boosted Tree hyperparameters\ngb_hyperparameters = {\n    'gradientboostingclassifier__n_estimators': [100, 200],\n    'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n    'gradientboostingclassifier__max_depth': [1, 3, 5]\n}\nhyperparameters = {\n  \n    'gb' : gb_hyperparameters\n}\n# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')\npred = fitted_models['gb'].predict(x)\n# Predict PROBABILITIES using Gradient Boosting\npred = fitted_models['gb'].predict_proba(x)\n\n# Get just the prediction for the positive class (1)\npred = [p[1] for p in pred]\n\n# # Display first 10 predictions\npred[:10]\npred = (pd.DataFrame(pred))\ndf['Probability'] = pred\ndf.head()\n# list_d1 = pd.DataFrame(fitted_models['gb'].best_estimator_.steps[1][1].feature_importances_)\n# list_d2=pd.DataFrame(list(X_test))\n# feature_importances= pd.concat([list_d1,list_d2], axis=1)\n# feature_importances.columns.values[1] = '1'\n# feature_importances = feature_importances.sort_values(by=[0],ascending=True)\n# feature_importances.columns=['Importance Score','Feature Name']\n# feature_importances=feature_importances.set_index('Feature Name')\n# plt.style.use('seaborn-whitegrid')\n# ax = feature_importances[-15:].plot(kind='barh', title =\"Feature Importance\",figsize=(15,10),\n#                                     fontsize=12)\n# ax.set_xlabel(\"Importance Score\",fontsize=12)\n# ax.set_ylabel(\"Feature Name\",fontsize=12)\n# plt.show() ## Top 15 important features in gradient boosting model using training data\n# feature_importances\ndf = spark.createDataFrame(df)\n\ndf = df.withColumnRenamed(\"__of_Issues\", \"Issues\")\ndf = df.withColumnRenamed(\"__of_Activities\", \"Activities\")\ndf = df.withColumnRenamed(\"__of_Opportunities_Won\", \"Opportunities_Won\")\nsave(df)","id":"20211110-233544_774153327","user":"incorta_user","config":{"editorSetting":{"completionSupport":true,"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"results":{"msg":[{"data":"(420, 105, 420, 105)\n('gb', 'has been fitted.')\n","type":"TEXT"}],"code":"SUCCESS"},"apps":[],"status":"FINISHED"},{"jobName":"paragraph_1636587453948_376192100","settings":{"params":{},"forms":{}},"dateCreated":"2021-11-10 23:37:33.948","progressUpdateIntervalMs":500,"isDefaultParagraph":false,"text":"%pyspark\n","id":"20211110-233733_1520624809","user":"incorta_user","config":{"editorSetting":{"completionSupport":true,"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"dateUpdated":"2021-11-10 23:42:32.907","apps":[],"status":"FINISHED"}],"config":{"isZeppelinNotebookCronEnable":false},"interpretersIds":[],"info":{}}
